{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fb8c1e-72c4-47c7-a243-5ad2de9d5dc9",
   "metadata": {},
   "source": [
    "# TNM112 -- Lab 1\n",
    "Instructions about the different tasks are specified in this notebook. However, for more details, please see the PDF with lab instructions. Also see the lab report template for information on how to report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90918b35-22b9-4d79-86b5-ff59405c91c0",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "Look through the \"data_generator.py\" code to understand how the dataset is generated and plotted.\n",
    "\n",
    "The \"importlib\" library is used to enable reloading of a library each time the cell is executed (so that changes to the imported script will be visible without restarting the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfde8e-f191-427f-bb3d-f62b6271a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data_generator\n",
    "importlib.reload(data_generator)\n",
    "\n",
    "data = data_generator.DataGenerator()\n",
    "data.generate(dataset='linear', N_train=32, N_test=512, K=4, sigma=0.4)\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba20d7c-1bfe-4a36-ac6f-757b431da3d3",
   "metadata": {},
   "source": [
    "### Keras MLP\n",
    "Look through the \"keras_mlp.py\" code to understand how the model is setup and trained.\n",
    "\n",
    "#### Task 1.1\n",
    "Use a linear dataset with N_train = 512 and K = 2. Specify a network without hidden layers. Compare:\n",
    "* Training for 4 epochs with learning rate 1.0 and batch size 512\n",
    "* Training for 4 epochs with learning rate 1.0 and batch size 16\n",
    "\n",
    "Answer the following questions:\n",
    "* Which model is best at separating the two classes (provide the accuracy on the test set)? Why is this the case? (Hint: how many iterations of SGD are performed in the two cases?)\n",
    "* Why is it possible to do the classification without non-linear activation function (there's only a softmax activation)?\n",
    "\n",
    "#### Task 1.2\n",
    "Use a polar dataset with N_train = 512 and K = 2. Specify a network with one hidden layer with 5 neurons. Train for 20 epochs with learning rate 1.0 and batch size 16. Compare:\n",
    "* Using linear activation function\n",
    "* Using sigmoid activation function\n",
    "* Using relu activation function\n",
    "\n",
    "Answer the following questions:\n",
    "* Why does linear activation not work?\n",
    "* On average, what is the best classification accuracy that can be achieved with a linear activation function?\n",
    "* Can you find an explanation for the difference comparing sigmoid and relu activation?\n",
    "\n",
    "#### Task 1.3\n",
    "Use a polar dataset with N_train = 512, K = 5, and sigma=0.05. Specify a network with 10 hidden layers with 50 neurons each and relu activation. Try different combinations of hyper parameters to get the best classification accuracy. Examples could be:\n",
    "* Change mean and stddev of normal initialization.\n",
    "* Change learning rate and add some momentum in SGD.\n",
    "* Check the Keras documentation for 'ExponentialDecay', which can be used to specify decaying learning rate for SGD.\n",
    "* You can also tweak batch size and number of epochs.\n",
    "\n",
    "What combination worked best, and what was your best classification accuracy (on the test set)? Can you find any patterns in what combinations of hyper parameters work and doesn't work?\n",
    "\n",
    "#### Task 1.4\n",
    "Continue from previous task, but change initialization to 'glorot_normal' and optimizer to 'keras.optimizers.Adam()'. Does this perform better compared to your results in Task 1.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d832e94-cea4-4037-a4aa-be533e8abd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras_mlp\n",
    "importlib.reload(keras_mlp) \n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Uncomment to be able to change data specification (instead of having to run the above cell every time)\n",
    "data.generate(dataset='linear', N_train=512, N_test=512, K=2, sigma=0.1) # Task 1.1\n",
    "#data.generate(dataset='polar', N_train=512, N_test=512, K=2, sigma=0.1)  # Task 1.2\n",
    "#data.generate(dataset='polar', N_train=512, N_test=512, K=5, sigma=0.05) # Task 1.3\n",
    "\n",
    "# Hyper-parameters\n",
    "hidden_layers = 0     # The number of hidden layers in the network (total number of layers will be L=hidden_layers+1)\n",
    "layer_width = 5       # The number of neurons in each hidden layer\n",
    "activation = 'linear' # Activation function of hidden layers\n",
    "init = keras.initializers.RandomNormal(mean=0.1, stddev=0.1) # Initialization method (starting point for the optimization)\n",
    "epochs = 4            # Number of epochs for the training\n",
    "batch_size = 512      # Batch size to use in training\n",
    "loss = keras.losses.MeanSquaredError() # Loss function\n",
    "opt = keras.optimizers.SGD(learning_rate=1.0, momentum=0.0) # Optimizer\n",
    "\n",
    "# The Keras MLP model\n",
    "model_k = keras_mlp.KerasMLP(data, verbose=True)\n",
    "\n",
    "# Setup the model with the specified hyper parameters\n",
    "model_k.setup_model(hidden_layers=hidden_layers, layer_width=layer_width,\n",
    "                    activation=activation, init=init)\n",
    "\n",
    "# Compile the model with a loss function and optimizer\n",
    "model_k.compile(loss_fn=loss, optimizer=opt)\n",
    "\n",
    "# Train the model a certain number of epochs with a specified batch size\n",
    "model_k.train(epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Plot the training progress\n",
    "model_k.plot_training()\n",
    "\n",
    "# Evaluate the model (loss and accuracy on the training and test data)\n",
    "model_k.evaluate()\n",
    "\n",
    "# Plot the dataset with decision boundaries generated by the trained model\n",
    "data.plot_classifier(model_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00021e7-4fae-4e32-9a85-ab74a42bd03c",
   "metadata": {},
   "source": [
    "### Our MLP\n",
    "To make sure our implementation of the MLP works as intended, we can run inference using the weights we trained with Keras. Then we can see if we get the same results in terms of accuracy and the decision boundaries generated by the model.\n",
    "\n",
    "#### Task 2\n",
    "Implement the \"activation\", \"setup_model\", \"feedforward\", and \"evaluate\" functions in \"mlp.py\". Run this cell and compare the results to the results generated by the Keras model in the previous cell (compare number of weights, classification loss/accuracy, and the plots with decision boundaries). Test this for different specifications of the model in the previous cell (number of hidden layers, layer width, and activation function)\n",
    "\n",
    "#### Task 3\n",
    "Use the linear dataset with 2 classes (K=2). Change the specification of the model parameters. Instead of extracting the weights from the Keras model, you should manually specify a weight matrix (2x2) and a bias vector (2x1), i.e. no hidden layers. Remember that each layer is supposed to be an item in a list, which means that you need to specify the weight matrix and the bias vector in separate 1-item lists. As we only have one layer, it will use the 'softmax' activation function.\n",
    "* Manually derive weights and biases to specify a model that draws a decision boundary at x2 = 1-x1. What is the simplest possible solution? How many additional solutions are possible?\n",
    "* How can you, in the simplest way, change the weights/biases to switch the predicted class labels?\n",
    "* By writing out the output of the MLP, motivate why your choice of weights and biases creates a decision boundary at x2 = 1-x1. Can you find a general formula for specifying which combinations of weights and biases will generate the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f915042-ad4a-422b-84b3-28b3a5891c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlp\n",
    "importlib.reload(mlp)\n",
    "\n",
    "# Get the weight matrices and biases of the trained Keras model\n",
    "W, b = model_k.get_weights()\n",
    "\n",
    "# Task 3: specify a weight matrix and a bias vector\n",
    "#W = TODO\n",
    "#b = TODO\n",
    "\n",
    "# This is our implementation of an MLP, which we set to use the dataset we generated\n",
    "model = mlp.MLP(data)\n",
    "\n",
    "# Assign the weights and biases to the MLP and specify the activation function\n",
    "model.setup_model(W, b, activation=activation)\n",
    "\n",
    "# Evaluate the model (accuracy on the training and test data)\n",
    "model.evaluate()\n",
    "\n",
    "# Plot the dataset with decision boundaries generated by our MLP\n",
    "data.plot_classifier(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
